<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Presentation</title>

		<meta name="description" content="2024">

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides" style="font-size:40px">

				<!-- New slide -->
				<section style="font-size:40px">
					<h3 style="color: #ffffff;">SONDRA WORKSHOP 2025</h3>
					<h3 style="color: #ffffff;">https://sondra2025.sciencesconf.org/</h3><br>
					<h2 style="color: #a8d2ff;">Research at NUS on optical and SAR data registration<br>using deep learning models </h2><br>
					<div style="color: #ffffff;">Michael Dell'aiera, Koen Mouthaan</div>

					<div class="container" style="margin:10px 120px 10px 120px">
						<div class="title" style="border-radius: 10px; background-color:rgb(255, 255, 255, 1);">
							<img src="resources/logos/logo_nus.png" style="height: 100px;"/>
							<img src="resources/logos/logo_sondra.png" style="height: 100px;"/>
						</div>
					</div>

					<div style="color: #ffffff;">May 14th 2025</div>
					<div style="color: #ffffff;">mda@nus.edu.sg</div>
				</section>

				<!-- New slide -->
				<section>
					<h2 class='slide-title' style="color:#9ecdff" align="left">Introduction</h2><hr>

					<p data-markdown style="text-align: left">
						Postdoc at NUS, Singapore on Optical and SAR data fusion using deep learning techniques
						<br>
						PhD work on deep learning applied to astrophysics (CTAO project)
					</p>

					<br>
					<div style="display: flex;">
						<div style="width: 1%"></div>
						<div style="width: 24%">
							Topic 1<br>SAR and Optical Registration
							<hr>
							<figure>
								<img style="width: 100%" src="resources/registration.png">
							</figure>
							<p style="color: red;">&rarr; Topic of the day</p>
						</div>
						<div style="width: 2%"></div>
						<div style="width: 24%">
							Topic 2<br>Optical-guided SAR Despeckling
							<hr>
							<figure>
								<img style="width: 100%" src="resources/despeckling.gif">
							</figure>
						</div>
						<div style="width: 2%"></div>
						<div style="width: 24%">
							Topic 3<br>Target Detection in SAR Images
							<hr>
							<figure>
								<img style="width: 100%" src="resources/target_detection.png">
							</figure>
							In collabaration with<br>Xu Xiaowo (NUS)
						</div>
						<div style="width: 2%"></div>
						<div style="width: 24%">
							Topic 4<br>Free Topics to Explore in Infomation Fusion / SAR
							<hr>
							<ul>
								<li>Scintillations, following the works of Harsha (and now Hugo)</li>
								<li>Complex-Valued Neural Network</li>
								<li>...</li>
							</ul>
						</div>
						<div style="width: 1%"></div>
				</section>
				
				<!-- New slide -->
				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Definition and contextualisation</p></h2><hr>

					<div style="display: flex;">
						<div style="width: 55%" align="left">
							<div>
								<ul>
									<li>Hundreds of remote sensing satellite launched between 1962 and 2014 [1]</li>
									<li>Collect information from different modalities</li>
								</ul>	
								<br>
								&rarr; Makes possible information fusion to link pixel information with signal and physical properties
								<br><br>
								<ul>
									<li>Different spatial resolution</li>
									<li>Geometric and radiometric differences</li>
								</ul>
								<br>
								&rarr; Scale, rotation, translation, occlusion, noise, etc.

								<br><br>
								<u>Objective of the presentation:</u>
								<ul>
									<li>Evaluate a popular deep learning method (CycleGAN) for image registration</li>
									<li>Assess domain adaptation and compare</li>
								</ul>
							</div>
						</div>
						<div style="width: 45%" align="left">
							Image registration = Process of aligning two or more images, i.e. find the transformation that link them.
							<figure>
								<img style="width: 80%" src="resources/image_registration_sift.png">
								<figcaption>Fig. Optical-optical image registration [2]</figcaption>
							</figure>
						</div>
					</div>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[1] [A survey of remote-sensing big data, Liu, 2015](https://www.researchgate.net/publication/281529343_A_survey_of_remote-sensing_big_data)

						[2] [A Survey on SAR and Optical Satellite Image Registration, Sommervold et al, 2023.](https://www.mdpi.com/2072-4292/15/3/850)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">State-of-the-art of Image Registration (Non-exhaustive)</p></h2><hr>

					<object type="image/svg+xml" data="resources/sota_registration.drawio.svg" width="100%"></object>

					<table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; text-align: left;">
						<thead>
						  <tr>
							<th>Category</th>
							<th>Pros</th>
							<th>Cons</th>
						  </tr>
						</thead>
						<tbody>
						  <tr>
							<td><strong>Area-Based</strong></td>
							<td>Simple, fast (Intensity)</td>
							<td>Fail with complex transformations</td>
						  </tr>
						  <tr>
							<td><strong>Hand-Designed</strong></td>
							<td>Robust to scale/rotation, interpretable</td>
							<td>Less robust in low-texture/repetitive areas, no semantic insight</td>
						  </tr>
						  <tr>
							<td><strong>Deep Learning</strong></td>
							<td>Highly robust, context-aware</td>
							<td>Requires training, overfitting</td>
						  </tr>
						</tbody>
					  </table>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Superpoint</p></h2><hr>

					<div style="display: flex;">
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 90%" src="resources/superpoint.png">
								<figcaption>Fig. Architecture of the SuperPoint</figcaption>
							</figure>
						</div>
						<div style="width: 50%">
							<p data-markdown style="text-align: left;">
								* Self-supervised learning of keypoints and descriptors
								* Two networks: Keypoint detector and descriptor
								* Susceptible to domain shifts

								&rarr; Need for Image-translation / domain adaptation
							</p>
						</div>
					</div>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[SuperPoint: Self-Supervised Interest Point Detection and Description, DeTone et al., 2017](https://arxiv.org/abs/1712.07629)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Image-to-image Translation (CycleGAN)</p></h2><hr>

					<figure>
						<img style="width: 75%" src="resources/cyclegan.png">
					</figure>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Domain adaptation</p></h2><hr>

					<p data-markdown style="text-align: left;">
						Transfer knowledge from a source domain to a target domain
						* Idea: we do not need all the information from the distributions
						* Very wide state-of-the-art [1]
					</p>
					
					<div style="display: flex;">
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 100%" src="resources/domain_adaptation_classification.png">
								<figcaption>Fig. Domain Adaptation in Classification [2]</figcaption>
							</figure>
						</div>
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 100%" src="resources/domain_adaptation_regression.png">
								<figcaption>Fig. Domain Adaptation in Regression [2]</figcaption>
							</figure>
						</div>
					</div>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[1] [A Review of Single-Source Deep Unsupervised Visual Domain Adaptation, Zhao, 2020](https://arxiv.org/abs/2009.00155)

						[2] [Unsupervised Domain Adaptation for Regression Using Dictionary Learning, Dhaini et al., 2023](https://normandie-univ.hal.science/hal-04012551v1/file/Unsupervised_Domain_Adaptation_Using_Dictionary_Learning__HAL_.pdf)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Domain Adversarial Neural Network (DANN)</p></h2><hr>
					
					<div style="display: flex;">
						<div style="width: 40%">
							<p data-markdown style="text-align: left;">
								* One of the most popular method
								* Unpaired images, unsupervised
								* Introduces Gradient Reversal Layer (GRL) and domain discriminator to compute an estimate of the $\mathcal{A}$-distance
								* Train an encoder to extract domain-invariant features
								* $\mathcal{H}_y \\subseteq \mathcal{H}_d$
								* Multi-task balancing
							</p>
						</div>
						<div style="width: 60%">
							<figure align="center">
								<img style="width: 100%" src="resources/dann.png">
								<figcaption>Fig. Architecture of the DANN</figcaption>
							</figure>
						</div>
					</div>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[Domain-Adversarial Training of Neural Networks, Ganin et al., 2015](https://arxiv.org/abs/1505.07818)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Preliminary Results (CycleGAN+SuperPoint)</p></h2><hr>
					
					Sentinel-1 (SAR) and Sentinel-2 (optical) with spatial resolution of 
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Preliminary Results (DANN+SuperPoint)</p></h2><hr>
					

				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Summary</p></h2><hr>
					
					Tableau recapitulatif des résultats obtenus sur les deux méthodes de registration, avantages et inconvénients de chacune d'entre elles
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Conclusion and Perspectives</p></h2><hr>
					
					<p data-markdown style="text-align: left">
						* Image translation methods "hallucinate", i.e. they create new features that are not present in the original image

						* Domain adaptation methods are difficult to train, especially adversarial methods

						&rarr; Wasserstein-DANN with Optimal Transport based distance computation

						* Other domain adaptation methods to explore, e.g. Optimal Transport

						&rarr; May 15th, SP.4: Optimal Transport for Radar Domain Adaptation, by Daniel Brooks 

						* Likelihood-free inference (likelihood-ratio estimation): Recasting a regression-style inference problem as a classification problem in order to sidestep the need for a likelihood ($\mathcal{H}_y \\subseteq \mathcal{H}_d$)

						* Test on different resolutions
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Annex: Despeckling</p></h2><hr>

					<figure>
						<img style="width: 70%" src="resources/despeckling.png">
					</figure>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Notes</p></h2><hr>
				</section>
			</div>
		</div>

		<!-- To add color to code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" />

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,
				progress: true,
				width: 1920,
  				height: 1080,
				center: false,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ],
			});
		</script>
	</body>
</html>
