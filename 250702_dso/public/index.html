<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Presentation</title>

		<meta name="description" content="2024">

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides" style="font-size:40px">

				<!-- New slide -->
				<section style="font-size:40px">
					<br><br><br>
					<h3 style="color: #ffffff;">DSO Progress Report</h3>
					<h2 style="color: #a8d2ff;">Research at NUS on optical and SAR data registration<br>using deep learning models</h2><br>
					<div style="color: #ffffff;">Michael Dell'aiera, Koen Mouthaan</div>

					<div class="container" style="margin:10px 120px 10px 120px">
						<div class="title" style="border-radius: 10px; background-color:rgb(255, 255, 255, 1);">
							<img src="resources/logos/logo_nus.png" style="height: 100px;"/>
							<img src="resources/logos/logo_dso.png" style="height: 100px;"/>
						</div>
					</div>

					<div style="color: #ffffff;">July 2nd, 2025</div>
					<div style="color: #ffffff;">mda@nus.edu.sg</div>
				</section>

				<!-- New slide -->
				<section>
					<h2 class='slide-title' style="color:#9ecdff" align="left">Workflow</h2><hr>

					<figure>
						<img style="width: 90%" src="resources/workflow.png">
					</figure>
				</section>
				
				<!-- New slide -->
				 <section data-transition="fade-in fade-out">
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Image registration</p></h2><hr>

					Process of aligning two or more images of the same scene taken at different times, viewpoints, or sensors.<br>

					<figure>
						<img style="height: 700px" src="resources/method0_superpoint.png">
						<figcaption>Fig. SuperPoint architecture [1]</figcaption>
					</figure>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[1] [SuperPoint: Self-Supervised Interest Point Detection and Description, DeTone et al., 2017](https://arxiv.org/abs/1712.07629)
					</p>
				</section>

				<section data-transition="fade-in fade-out">
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Image registration</p></h2><hr>

					Process of aligning two or more images of the same scene taken at different times, viewpoints, or sensors.<br>

					<figure>
						<img style="height: 700px" src="resources/method1_image_translation.png">
						<figcaption>Fig. Method 1: Image translation</figcaption>
					</figure>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[1] [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)
					</p>
				</section>

				<section data-transition="fade-in fade-out">
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Image registration</p></h2><hr>

					Process of aligning two or more images of the same scene taken at different times, viewpoints, or sensors.<br>

					<figure>
						<img style="height: 700px" src="resources/method2_domain_adaptation.png">
						<figcaption>Fig. Method 2: Domain adaptation</figcaption>
					</figure>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[Domain-Adversarial Training of Neural Networks, Ganin et al., 2015](https://arxiv.org/abs/1505.07818)
					</p>
				</section>

				<section data-transition="fade-in fade-out">
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Image registration</p></h2><hr>

					Process of aligning two or more images of the same scene taken at different times, viewpoints, or sensors.<br>

					<figure>
						<img style="height: 700px" src="resources/method3_keypoint_selection.png">
						<figcaption>Fig. Method 3: Keypoint selection</figcaption>
					</figure>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Keypoint matching</p></h2><hr>

					<div style="display: flex;">
						<div style="width: 40%">
							<figure>
								<img style="height: 350px" src="resources/superpoint_matched_keypoints.png">
								<figcaption style="margin-top: -0.8em;">Fig. Keypoint matching (radius=5)</figcaption>
							</figure>
							<br>
							<figure>
								<img style="height: 350px" src="resources/superpoint_descriptor_importance.png">
								<figcaption style="margin-top: -0.8em;">Fig. Keypoint weight (red=high, blue=low)</figcaption>
							</figure>
						</div>

						<div style="width: 5%">

						</div>

						<div style="width: 55%">
							<figure>
								<img style="height: 500px" src="resources/superpoint_keypoints_vs_radius.png">
								<figcaption style="margin-top: -0.8em;">Fig. % of keypoints as a function of the radius</figcaption>
							</figure>

							<br><br>
							<div style="text-align: left;">
								Calculated on the QXS-SAROPT dataset [1]
							</div>
							<div style="text-align: left; color: rgb(255, 67, 67);">
								For a radius r=5, ~20% of keypoints are consistently matched
							</div>

							<hr>
							<p data-markdown style="text-align: left; font-size:20px">
								[1] [The QXS-SAROPT Dataset for Deep Learning in SAR-Optical Data Fusion, Huang et al., 2021](https://arxiv.org/abs/2103.08259)
							</p>
						</div>
					</div>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Preliminary Results (CycleGAN+SuperPoint)</p></h2><hr>

					<div style="display: flex;">
						<div style="width: 25%">
							<figure align="center">
								<img style="width: 75%" src="resources/cyclegan_real_B.png">
								<figcaption style="margin-top: -0.8em;">Real optical</figcaption>
							</figure>
						</div>
						<div style="width: 25%">
							<figure align="center">
								<img style="width: 75%" src="resources/cyclegan_fake_B.png">
								<figcaption style="margin-top: -0.8em;">Translated SAR</figcaption>
							</figure>
						</div>
						<div style="width: 25%">
							<figure align="center">
								<img style="width: 75%" src="resources/cyclegan_real_A.png">
								<figcaption style="margin-top: -0.8em;">Real SAR</figcaption>
							</figure>
						</div>
						<div style="width: 25%">
							<figure align="center">
								<img style="width: 75%" src="resources/cyclegan_fake_A.png">
								<figcaption style="margin-top: -0.8em;">Translated optical</figcaption>
							</figure>
						</div>
					</div>

					<div style="display: flex;">
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 80%" src="resources/cyclegan_opt_opt.png">
								<figcaption style="margin-top: -0.8em;">Fig. SAR to Optical</figcaption>
							</figure>
						</div>
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 80%" src="resources/cyclegan_sar_sar.png">
								<figcaption style="margin-top: -0.8em;">Fig. Optical to SAR</figcaption>
							</figure>
						</div>
					</div>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Preliminary Results (DANN+SuperPoint)</p></h2><hr>
					
					<div style="display: flex;">
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 100%" src="resources/dann_matches_before_adaptation.png">
								<figcaption style="margin-top: -0.8em;">Fig. Before adaptation</figcaption>
							</figure>
						</div>
						<div style="width: 50%">
							<figure align="center">
								<img style="width: 100%" src="resources/dann_matches_after_adaptation.png">
								<figcaption style="margin-top: -0.8em;">Fig. After adaptation</figcaption>
							</figure>
						</div>
					</div>

					<br><br>
					Improvment of 10% in the number of matching keypoints after adaptation compared to image translation
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Despeckling</p></h2><hr>

					<div style="display: flex;">
						<div style="width: 50%">
							<p data-markdown style="text-align: left">
								* Inspired from [1]
									* Bilateral filtering using both optical and SAR luminance

								* Texture-descriptor based on [2]
									* 1st and 2nd order statistics computed on neighborhood of each pixel (radius r)
									* Unnormalized bilateral filtering of the descriptor
									* K-Means or distance metric
							</p>
						</div>

						<div style="width: 50%">
							<figure>
								<img style="width: 70%" src="resources/despeckling_workflow.png">
								<figcaption>Fig. Workflow if the optical-guided despeckling process</figcaption>
							</figure>
						</div>
					</div>

					<hr>
					<p data-markdown style="text-align: left; font-size:20px">
						[1] [SAR despeckling guided by an optical image, Verdoliva et al., 2014](https://ieeexplore.ieee.org/document/6947286)

						[2] [Local texture-based color transfer and colorization, Arbelot et al., 2016](https://hal.science/hal-01246615/file/RR-8834.pdf)
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Despeckling</p></h2><hr>

					<figure>
						<img style="width: 100%" src="resources/despeckling_kmeans.png">
						<figcaption style="margin-top: -0.8em;">Fig. Optical-guided despeckling using K-means. From left to right: optical, SAR, filtered SAR, filtered and colorised SAR.</figcaption>
					</figure>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Despeckling</p></h2><hr>

					<figure>
						<img style="width: 100%" src="resources/despeckling_filtered.png">
						<figcaption style="margin-top: -0.8em;">Fig. Optical-guided despeckling using distance metric. From left to right: optical, SAR, filtered SAR, filtered and colorised SAR.</figcaption>
					</figure>

					\[ \begin{aligned} \mathcal{D}_{\sigma_d}(p, q) &amp; =  \exp{\left(-\dfrac{||S(p)-S(q)||^2}{2 \sigma_d^2}\right)} \\
					\mu(p) &amp; =  Z^{-1} \sum_{q} L_{SAR}(q) \mathcal{D}_{\sigma_d}(p, q) \end{aligned} \]
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Conclusion and Perspectives</p></h2><hr>
					
					<p data-markdown style="text-align: left">
						* Image Registration is very difficult because of geometric and radiometric differences
						
							* Image translation methods "hallucinate", i.e. they create new features that are not present in the original image

							* Domain adaptation methods are difficult to train, especially adversarial methods

							&rarr; Wasserstein-DANN with Optimal Transport based distance computation

							&rarr; Keypoint selection methods to be implemented

						* Implementation of a optical-guided texture-based despeckling strategy

							* Possibility to tailor the descriptor with SAR-based information

							* Need for comparison with MERLIN, SAR2SAR, and other despeckling methods

							* Small test dataset creation containing complex SAR and co-registered optical images
					</p>
				</section>

				<section>
					<h2 class='slide-title' align="left"><p style="color:#9ecdff">Notes</p></h2><hr>
				</section>
			</div>
		</div>

		<!-- To add color to code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" />

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,
				progress: true,
				width: 1920,
  				height: 1080,
				center: false,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ],
			});
		</script>
	</body>
</html>
